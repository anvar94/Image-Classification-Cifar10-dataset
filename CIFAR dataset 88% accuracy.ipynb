{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "#from scipy.misc import toimage\n",
    "from keras.datasets import cifar10\n",
    "\n",
    " \n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator( rotation_range=90,\n",
    "                 width_shift_range=0.1, height_shift_range=0.1,\n",
    "                 horizontal_flip=True)\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import numpy as np\n",
    " \n",
    "def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    if epoch > 100:\n",
    "        lrate = 0.0003\n",
    "    return lrate\n",
    " \n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    " \n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test = (x_test-mean)/(std+1e-7)\n",
    " \n",
    "num_classes = 10\n",
    "y_train = np_utils.to_categorical(y_train,num_classes)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes)\n",
    " \n",
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    " \n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    " \n",
    "model.summary()\n",
    " \n",
    "#data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    )\n",
    "datagen.fit(x_train)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "781/781 [==============================] - 298s 382ms/step - loss: 1.9193 - accuracy: 0.4246 - val_loss: 1.3346 - val_accuracy: 0.5799\n",
      "Epoch 2/125\n",
      "781/781 [==============================] - 306s 392ms/step - loss: 1.2920 - accuracy: 0.5902 - val_loss: 1.0911 - val_accuracy: 0.6670\n",
      "Epoch 3/125\n",
      "781/781 [==============================] - 299s 383ms/step - loss: 1.0917 - accuracy: 0.6518 - val_loss: 0.9532 - val_accuracy: 0.7001\n",
      "Epoch 4/125\n",
      "781/781 [==============================] - 291s 372ms/step - loss: 0.9794 - accuracy: 0.6899 - val_loss: 0.9880 - val_accuracy: 0.7130\n",
      "Epoch 5/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.9123 - accuracy: 0.7146 - val_loss: 0.9161 - val_accuracy: 0.7271\n",
      "Epoch 6/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.8642 - accuracy: 0.7332 - val_loss: 0.9792 - val_accuracy: 0.7187\n",
      "Epoch 7/125\n",
      "781/781 [==============================] - 293s 376ms/step - loss: 0.8315 - accuracy: 0.7503 - val_loss: 0.8332 - val_accuracy: 0.7595\n",
      "Epoch 8/125\n",
      "781/781 [==============================] - 293s 376ms/step - loss: 0.8075 - accuracy: 0.7582 - val_loss: 0.7416 - val_accuracy: 0.7890\n",
      "Epoch 9/125\n",
      "781/781 [==============================] - 296s 379ms/step - loss: 0.7836 - accuracy: 0.7684 - val_loss: 0.7800 - val_accuracy: 0.7853\n",
      "Epoch 10/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.7584 - accuracy: 0.7773 - val_loss: 0.6857 - val_accuracy: 0.8099\n",
      "Epoch 11/125\n",
      "781/781 [==============================] - 296s 378ms/step - loss: 0.7498 - accuracy: 0.7833 - val_loss: 0.8337 - val_accuracy: 0.7714\n",
      "Epoch 12/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.7321 - accuracy: 0.7886 - val_loss: 0.6958 - val_accuracy: 0.8119\n",
      "Epoch 13/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.7213 - accuracy: 0.7942 - val_loss: 0.6983 - val_accuracy: 0.8134\n",
      "Epoch 14/125\n",
      "781/781 [==============================] - 294s 377ms/step - loss: 0.7083 - accuracy: 0.7976 - val_loss: 0.6522 - val_accuracy: 0.8252\n",
      "Epoch 15/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.6986 - accuracy: 0.8022 - val_loss: 0.6877 - val_accuracy: 0.8206\n",
      "Epoch 16/125\n",
      "781/781 [==============================] - 293s 376ms/step - loss: 0.6929 - accuracy: 0.8047 - val_loss: 0.6456 - val_accuracy: 0.8322\n",
      "Epoch 17/125\n",
      "781/781 [==============================] - 291s 373ms/step - loss: 0.6854 - accuracy: 0.8084 - val_loss: 0.7241 - val_accuracy: 0.8080\n",
      "Epoch 18/125\n",
      "781/781 [==============================] - 292s 374ms/step - loss: 0.6761 - accuracy: 0.8132 - val_loss: 0.7226 - val_accuracy: 0.8055\n",
      "Epoch 19/125\n",
      "781/781 [==============================] - 292s 374ms/step - loss: 0.6747 - accuracy: 0.8122 - val_loss: 0.6140 - val_accuracy: 0.8381\n",
      "Epoch 20/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.6647 - accuracy: 0.8158 - val_loss: 0.6440 - val_accuracy: 0.8331\n",
      "Epoch 21/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.6628 - accuracy: 0.8184 - val_loss: 0.7000 - val_accuracy: 0.8145\n",
      "Epoch 22/125\n",
      "781/781 [==============================] - 297s 380ms/step - loss: 0.6533 - accuracy: 0.8214 - val_loss: 0.6299 - val_accuracy: 0.8397\n",
      "Epoch 23/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.6542 - accuracy: 0.8210 - val_loss: 0.6015 - val_accuracy: 0.8487\n",
      "Epoch 24/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.6521 - accuracy: 0.8239 - val_loss: 0.6098 - val_accuracy: 0.8421\n",
      "Epoch 25/125\n",
      "781/781 [==============================] - 294s 377ms/step - loss: 0.6441 - accuracy: 0.8248 - val_loss: 0.6160 - val_accuracy: 0.8416\n",
      "Epoch 26/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.6413 - accuracy: 0.8278 - val_loss: 0.6734 - val_accuracy: 0.8254\n",
      "Epoch 27/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.6389 - accuracy: 0.8291 - val_loss: 0.6208 - val_accuracy: 0.8368\n",
      "Epoch 28/125\n",
      "781/781 [==============================] - 299s 382ms/step - loss: 0.6358 - accuracy: 0.8300 - val_loss: 0.5851 - val_accuracy: 0.8475\n",
      "Epoch 29/125\n",
      "781/781 [==============================] - 295s 378ms/step - loss: 0.6341 - accuracy: 0.8315 - val_loss: 0.6995 - val_accuracy: 0.8250\n",
      "Epoch 30/125\n",
      "781/781 [==============================] - 296s 379ms/step - loss: 0.6347 - accuracy: 0.8321 - val_loss: 0.6826 - val_accuracy: 0.8285\n",
      "Epoch 31/125\n",
      "781/781 [==============================] - 296s 378ms/step - loss: 0.6258 - accuracy: 0.8350 - val_loss: 0.6424 - val_accuracy: 0.8334\n",
      "Epoch 32/125\n",
      "781/781 [==============================] - 296s 379ms/step - loss: 0.6247 - accuracy: 0.8345 - val_loss: 0.5937 - val_accuracy: 0.8527\n",
      "Epoch 33/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.6271 - accuracy: 0.8341 - val_loss: 0.6107 - val_accuracy: 0.8495\n",
      "Epoch 34/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.6213 - accuracy: 0.8365 - val_loss: 0.6213 - val_accuracy: 0.8459\n",
      "Epoch 35/125\n",
      "781/781 [==============================] - 292s 374ms/step - loss: 0.6230 - accuracy: 0.8369 - val_loss: 0.6314 - val_accuracy: 0.8413\n",
      "Epoch 36/125\n",
      "781/781 [==============================] - 293s 376ms/step - loss: 0.6168 - accuracy: 0.8395 - val_loss: 0.6358 - val_accuracy: 0.8391\n",
      "Epoch 37/125\n",
      "781/781 [==============================] - 294s 377ms/step - loss: 0.6179 - accuracy: 0.8383 - val_loss: 0.6555 - val_accuracy: 0.8331\n",
      "Epoch 38/125\n",
      "781/781 [==============================] - 294s 377ms/step - loss: 0.6175 - accuracy: 0.8369 - val_loss: 0.5795 - val_accuracy: 0.8557\n",
      "Epoch 39/125\n",
      "781/781 [==============================] - 295s 377ms/step - loss: 0.6135 - accuracy: 0.8384 - val_loss: 0.5574 - val_accuracy: 0.8614\n",
      "Epoch 40/125\n",
      "781/781 [==============================] - 294s 377ms/step - loss: 0.6103 - accuracy: 0.8424 - val_loss: 0.6390 - val_accuracy: 0.8431\n",
      "Epoch 41/125\n",
      "781/781 [==============================] - 295s 378ms/step - loss: 0.6083 - accuracy: 0.8431 - val_loss: 0.6132 - val_accuracy: 0.8439\n",
      "Epoch 42/125\n",
      "781/781 [==============================] - 295s 378ms/step - loss: 0.6139 - accuracy: 0.8397 - val_loss: 0.6183 - val_accuracy: 0.8491\n",
      "Epoch 43/125\n",
      "781/781 [==============================] - 295s 378ms/step - loss: 0.6096 - accuracy: 0.8423 - val_loss: 0.6133 - val_accuracy: 0.8484\n",
      "Epoch 44/125\n",
      "781/781 [==============================] - 296s 379ms/step - loss: 0.6023 - accuracy: 0.8443 - val_loss: 0.6322 - val_accuracy: 0.8431\n",
      "Epoch 45/125\n",
      "781/781 [==============================] - 295s 378ms/step - loss: 0.6093 - accuracy: 0.8412 - val_loss: 0.6722 - val_accuracy: 0.8302\n",
      "Epoch 46/125\n",
      "781/781 [==============================] - 295s 378ms/step - loss: 0.6027 - accuracy: 0.8437 - val_loss: 0.5613 - val_accuracy: 0.8612\n",
      "Epoch 47/125\n",
      "781/781 [==============================] - 295s 378ms/step - loss: 0.5986 - accuracy: 0.8441 - val_loss: 0.5582 - val_accuracy: 0.8619\n",
      "Epoch 48/125\n",
      "781/781 [==============================] - 297s 381ms/step - loss: 0.5983 - accuracy: 0.8453 - val_loss: 0.6633 - val_accuracy: 0.8324\n",
      "Epoch 49/125\n",
      "781/781 [==============================] - 295s 377ms/step - loss: 0.6019 - accuracy: 0.8445 - val_loss: 0.5888 - val_accuracy: 0.8606\n",
      "Epoch 50/125\n",
      "781/781 [==============================] - 294s 377ms/step - loss: 0.6013 - accuracy: 0.8457 - val_loss: 0.6025 - val_accuracy: 0.8523\n",
      "Epoch 51/125\n",
      "781/781 [==============================] - 296s 379ms/step - loss: 0.5962 - accuracy: 0.8460 - val_loss: 0.6084 - val_accuracy: 0.8522\n",
      "Epoch 52/125\n",
      "781/781 [==============================] - 297s 381ms/step - loss: 0.5975 - accuracy: 0.8473 - val_loss: 0.7353 - val_accuracy: 0.8131\n",
      "Epoch 53/125\n",
      "781/781 [==============================] - 297s 380ms/step - loss: 0.5970 - accuracy: 0.8467 - val_loss: 0.6091 - val_accuracy: 0.8455\n",
      "Epoch 54/125\n",
      "781/781 [==============================] - 296s 378ms/step - loss: 0.5969 - accuracy: 0.8490 - val_loss: 0.5450 - val_accuracy: 0.8666\n",
      "Epoch 55/125\n",
      "781/781 [==============================] - 296s 379ms/step - loss: 0.5900 - accuracy: 0.8480 - val_loss: 0.5736 - val_accuracy: 0.8599\n",
      "Epoch 56/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 293s 375ms/step - loss: 0.5929 - accuracy: 0.8482 - val_loss: 0.6283 - val_accuracy: 0.8436\n",
      "Epoch 57/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.5914 - accuracy: 0.8494 - val_loss: 0.5841 - val_accuracy: 0.8568\n",
      "Epoch 58/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.5898 - accuracy: 0.8503 - val_loss: 0.5512 - val_accuracy: 0.8676\n",
      "Epoch 59/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.5896 - accuracy: 0.8485 - val_loss: 0.6540 - val_accuracy: 0.8376\n",
      "Epoch 60/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.5904 - accuracy: 0.8484 - val_loss: 0.6066 - val_accuracy: 0.8478\n",
      "Epoch 61/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.5903 - accuracy: 0.8498 - val_loss: 0.6083 - val_accuracy: 0.8493\n",
      "Epoch 62/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.5877 - accuracy: 0.8491 - val_loss: 0.6258 - val_accuracy: 0.8441\n",
      "Epoch 63/125\n",
      "781/781 [==============================] - 295s 378ms/step - loss: 0.5866 - accuracy: 0.8493 - val_loss: 0.6110 - val_accuracy: 0.8520\n",
      "Epoch 64/125\n",
      "781/781 [==============================] - 297s 380ms/step - loss: 0.5812 - accuracy: 0.8529 - val_loss: 0.5577 - val_accuracy: 0.8678\n",
      "Epoch 65/125\n",
      "781/781 [==============================] - 290s 372ms/step - loss: 0.5882 - accuracy: 0.8508 - val_loss: 0.6027 - val_accuracy: 0.8560\n",
      "Epoch 66/125\n",
      "781/781 [==============================] - 291s 372ms/step - loss: 0.5831 - accuracy: 0.8521 - val_loss: 0.6043 - val_accuracy: 0.8518\n",
      "Epoch 67/125\n",
      "781/781 [==============================] - 291s 372ms/step - loss: 0.5841 - accuracy: 0.8518 - val_loss: 0.6212 - val_accuracy: 0.8439\n",
      "Epoch 68/125\n",
      "781/781 [==============================] - 290s 372ms/step - loss: 0.5802 - accuracy: 0.8548 - val_loss: 0.6133 - val_accuracy: 0.8510\n",
      "Epoch 69/125\n",
      "781/781 [==============================] - 291s 373ms/step - loss: 0.5862 - accuracy: 0.8525 - val_loss: 0.5811 - val_accuracy: 0.8602\n",
      "Epoch 70/125\n",
      "781/781 [==============================] - 292s 374ms/step - loss: 0.5792 - accuracy: 0.8527 - val_loss: 0.6178 - val_accuracy: 0.8502\n",
      "Epoch 71/125\n",
      "781/781 [==============================] - 315s 403ms/step - loss: 0.5772 - accuracy: 0.8532 - val_loss: 0.6562 - val_accuracy: 0.8361\n",
      "Epoch 72/125\n",
      "781/781 [==============================] - 328s 420ms/step - loss: 0.5783 - accuracy: 0.8536 - val_loss: 0.6518 - val_accuracy: 0.8411\n",
      "Epoch 73/125\n",
      "781/781 [==============================] - 324s 415ms/step - loss: 0.5775 - accuracy: 0.8550 - val_loss: 0.6347 - val_accuracy: 0.8450\n",
      "Epoch 74/125\n",
      "781/781 [==============================] - 305s 390ms/step - loss: 0.5807 - accuracy: 0.8535 - val_loss: 0.5763 - val_accuracy: 0.8604\n",
      "Epoch 75/125\n",
      "781/781 [==============================] - 298s 381ms/step - loss: 0.5716 - accuracy: 0.8566 - val_loss: 0.6162 - val_accuracy: 0.8476\n",
      "Epoch 76/125\n",
      "781/781 [==============================] - 298s 381ms/step - loss: 0.5760 - accuracy: 0.8551 - val_loss: 0.5728 - val_accuracy: 0.8613\n",
      "Epoch 77/125\n",
      "781/781 [==============================] - 296s 378ms/step - loss: 0.5394 - accuracy: 0.8670 - val_loss: 0.5358 - val_accuracy: 0.8756\n",
      "Epoch 78/125\n",
      "781/781 [==============================] - 302s 387ms/step - loss: 0.5104 - accuracy: 0.8745 - val_loss: 0.5785 - val_accuracy: 0.8596\n",
      "Epoch 79/125\n",
      "781/781 [==============================] - 304s 389ms/step - loss: 0.5112 - accuracy: 0.8736 - val_loss: 0.5276 - val_accuracy: 0.8704\n",
      "Epoch 80/125\n",
      "781/781 [==============================] - 295s 377ms/step - loss: 0.4999 - accuracy: 0.8752 - val_loss: 0.5303 - val_accuracy: 0.8760\n",
      "Epoch 81/125\n",
      "781/781 [==============================] - 296s 379ms/step - loss: 0.4905 - accuracy: 0.8782 - val_loss: 0.5570 - val_accuracy: 0.8656\n",
      "Epoch 82/125\n",
      "781/781 [==============================] - 299s 383ms/step - loss: 0.4953 - accuracy: 0.8769 - val_loss: 0.5540 - val_accuracy: 0.8655\n",
      "Epoch 83/125\n",
      "781/781 [==============================] - 299s 382ms/step - loss: 0.4950 - accuracy: 0.8751 - val_loss: 0.5418 - val_accuracy: 0.8698\n",
      "Epoch 84/125\n",
      "781/781 [==============================] - 298s 381ms/step - loss: 0.4854 - accuracy: 0.8783 - val_loss: 0.5352 - val_accuracy: 0.8710\n",
      "Epoch 85/125\n",
      "781/781 [==============================] - 297s 380ms/step - loss: 0.4820 - accuracy: 0.8787 - val_loss: 0.5001 - val_accuracy: 0.8760\n",
      "Epoch 86/125\n",
      "781/781 [==============================] - 297s 380ms/step - loss: 0.4803 - accuracy: 0.8777 - val_loss: 0.5128 - val_accuracy: 0.8748\n",
      "Epoch 87/125\n",
      "781/781 [==============================] - 298s 381ms/step - loss: 0.4810 - accuracy: 0.8790 - val_loss: 0.5315 - val_accuracy: 0.8710\n",
      "Epoch 88/125\n",
      "781/781 [==============================] - 298s 382ms/step - loss: 0.4736 - accuracy: 0.8808 - val_loss: 0.5162 - val_accuracy: 0.8729\n",
      "Epoch 89/125\n",
      "781/781 [==============================] - 301s 385ms/step - loss: 0.4689 - accuracy: 0.8820 - val_loss: 0.4927 - val_accuracy: 0.8810\n",
      "Epoch 90/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.4696 - accuracy: 0.8805 - val_loss: 0.4843 - val_accuracy: 0.8845\n",
      "Epoch 91/125\n",
      "781/781 [==============================] - 292s 374ms/step - loss: 0.4684 - accuracy: 0.8803 - val_loss: 0.5271 - val_accuracy: 0.8719\n",
      "Epoch 92/125\n",
      "781/781 [==============================] - 292s 373ms/step - loss: 0.4651 - accuracy: 0.8816 - val_loss: 0.5017 - val_accuracy: 0.8792\n",
      "Epoch 93/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.4667 - accuracy: 0.8794 - val_loss: 0.4734 - val_accuracy: 0.8847\n",
      "Epoch 94/125\n",
      "781/781 [==============================] - 292s 374ms/step - loss: 0.4659 - accuracy: 0.8809 - val_loss: 0.5072 - val_accuracy: 0.8726\n",
      "Epoch 95/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.4608 - accuracy: 0.8828 - val_loss: 0.5051 - val_accuracy: 0.8745\n",
      "Epoch 96/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.4603 - accuracy: 0.8807 - val_loss: 0.5252 - val_accuracy: 0.8690\n",
      "Epoch 97/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.4608 - accuracy: 0.8822 - val_loss: 0.5068 - val_accuracy: 0.8754\n",
      "Epoch 98/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.4602 - accuracy: 0.8820 - val_loss: 0.5154 - val_accuracy: 0.8722\n",
      "Epoch 99/125\n",
      "781/781 [==============================] - 293s 376ms/step - loss: 0.4590 - accuracy: 0.8819 - val_loss: 0.4889 - val_accuracy: 0.8799\n",
      "Epoch 100/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.4528 - accuracy: 0.8828 - val_loss: 0.4968 - val_accuracy: 0.8782\n",
      "Epoch 101/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.4591 - accuracy: 0.8820 - val_loss: 0.5057 - val_accuracy: 0.8757\n",
      "Epoch 102/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.4366 - accuracy: 0.8889 - val_loss: 0.5010 - val_accuracy: 0.8747\n",
      "Epoch 103/125\n",
      "781/781 [==============================] - 294s 377ms/step - loss: 0.4287 - accuracy: 0.8915 - val_loss: 0.4822 - val_accuracy: 0.8815\n",
      "Epoch 104/125\n",
      "781/781 [==============================] - 295s 377ms/step - loss: 0.4200 - accuracy: 0.8936 - val_loss: 0.4710 - val_accuracy: 0.8860\n",
      "Epoch 105/125\n",
      "781/781 [==============================] - 295s 378ms/step - loss: 0.4223 - accuracy: 0.8915 - val_loss: 0.5038 - val_accuracy: 0.8769\n",
      "Epoch 106/125\n",
      "781/781 [==============================] - 295s 378ms/step - loss: 0.4236 - accuracy: 0.8913 - val_loss: 0.4560 - val_accuracy: 0.8881\n",
      "Epoch 107/125\n",
      "781/781 [==============================] - 296s 379ms/step - loss: 0.4157 - accuracy: 0.8933 - val_loss: 0.4855 - val_accuracy: 0.8806\n",
      "Epoch 108/125\n",
      "781/781 [==============================] - 295s 378ms/step - loss: 0.4179 - accuracy: 0.8943 - val_loss: 0.4615 - val_accuracy: 0.8867\n",
      "Epoch 109/125\n",
      "781/781 [==============================] - 295s 378ms/step - loss: 0.4122 - accuracy: 0.8936 - val_loss: 0.4686 - val_accuracy: 0.8819\n",
      "Epoch 110/125\n",
      "781/781 [==============================] - 295s 377ms/step - loss: 0.4100 - accuracy: 0.8954 - val_loss: 0.5082 - val_accuracy: 0.8744\n",
      "Epoch 111/125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 292s 374ms/step - loss: 0.4120 - accuracy: 0.8938 - val_loss: 0.4717 - val_accuracy: 0.8821\n",
      "Epoch 112/125\n",
      "781/781 [==============================] - 293s 376ms/step - loss: 0.4043 - accuracy: 0.8962 - val_loss: 0.4658 - val_accuracy: 0.8857\n",
      "Epoch 113/125\n",
      "781/781 [==============================] - 292s 373ms/step - loss: 0.4124 - accuracy: 0.8943 - val_loss: 0.4674 - val_accuracy: 0.8823\n",
      "Epoch 114/125\n",
      "781/781 [==============================] - 291s 373ms/step - loss: 0.4074 - accuracy: 0.8956 - val_loss: 0.4682 - val_accuracy: 0.8846\n",
      "Epoch 115/125\n",
      "781/781 [==============================] - 292s 373ms/step - loss: 0.4074 - accuracy: 0.8951 - val_loss: 0.4804 - val_accuracy: 0.8811\n",
      "Epoch 116/125\n",
      "781/781 [==============================] - 291s 373ms/step - loss: 0.4048 - accuracy: 0.8965 - val_loss: 0.4769 - val_accuracy: 0.8827\n",
      "Epoch 117/125\n",
      "781/781 [==============================] - 291s 373ms/step - loss: 0.4045 - accuracy: 0.8953 - val_loss: 0.4721 - val_accuracy: 0.8832\n",
      "Epoch 118/125\n",
      "781/781 [==============================] - 298s 382ms/step - loss: 0.4080 - accuracy: 0.8944 - val_loss: 0.4448 - val_accuracy: 0.8902\n",
      "Epoch 119/125\n",
      "781/781 [==============================] - 294s 376ms/step - loss: 0.4021 - accuracy: 0.8963 - val_loss: 0.4479 - val_accuracy: 0.8896\n",
      "Epoch 120/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.3991 - accuracy: 0.8976 - val_loss: 0.4521 - val_accuracy: 0.8879\n",
      "Epoch 121/125\n",
      "781/781 [==============================] - 294s 377ms/step - loss: 0.3966 - accuracy: 0.8978 - val_loss: 0.4628 - val_accuracy: 0.8831\n",
      "Epoch 122/125\n",
      "781/781 [==============================] - 292s 374ms/step - loss: 0.4033 - accuracy: 0.8955 - val_loss: 0.4810 - val_accuracy: 0.8817\n",
      "Epoch 123/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.3919 - accuracy: 0.8984 - val_loss: 0.4413 - val_accuracy: 0.8895\n",
      "Epoch 124/125\n",
      "781/781 [==============================] - 293s 375ms/step - loss: 0.3987 - accuracy: 0.8956 - val_loss: 0.4441 - val_accuracy: 0.8881\n",
      "Epoch 125/125\n",
      "781/781 [==============================] - 291s 373ms/step - loss: 0.3914 - accuracy: 0.8967 - val_loss: 0.4850 - val_accuracy: 0.8810\n",
      "10000/10000 [==============================] - 16s 2ms/step\n",
      "\n",
      "Test result: 88.100 loss: 0.485\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "batch_size = 64\n",
    " \n",
    "opt_rms = keras.optimizers.rmsprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
    "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
    "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
    "#save to disk\n",
    "model_json = model.to_json()\n",
    "with open('model.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('model.h5') \n",
    " \n",
    "#testing\n",
    "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
