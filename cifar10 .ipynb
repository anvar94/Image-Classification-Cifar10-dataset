{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-ef10a30a08e3>:61: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-2-ef10a30a08e3>:86: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Gachon\\Anaconda3\\envs\\tf_1.4\\lib\\site-packages\\tensorflow\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "(Epoch): 0, training data percent: 0.093750, loss: 182.081146\n",
      "(Epoch): 100, training data percent: 0.265625, loss: 2.026101\n",
      "(Epoch): 200, training data percent: 0.320312, loss: 1.789721\n",
      "(Epoch): 300, training data percent: 0.398438, loss: 1.700730\n",
      "(Epoch): 400, training data percent: 0.335938, loss: 1.717653\n",
      "(Epoch): 500, training data percent: 0.382812, loss: 1.607475\n",
      "(Epoch): 600, training data percent: 0.429688, loss: 1.421599\n",
      "(Epoch): 700, training data percent: 0.492188, loss: 1.481823\n",
      "(Epoch): 800, training data percent: 0.492188, loss: 1.542272\n",
      "(Epoch): 900, training data percent: 0.484375, loss: 1.360296\n",
      "(Epoch): 1000, training data percent: 0.585938, loss: 1.361492\n",
      "(Epoch): 1100, training data percent: 0.609375, loss: 1.084651\n",
      "(Epoch): 1200, training data percent: 0.625000, loss: 1.050969\n",
      "(Epoch): 1300, training data percent: 0.546875, loss: 1.281939\n",
      "(Epoch): 1400, training data percent: 0.640625, loss: 1.059899\n",
      "(Epoch): 1500, training data percent: 0.609375, loss: 1.121250\n",
      "(Epoch): 1600, training data percent: 0.632812, loss: 1.106532\n",
      "(Epoch): 1700, training data percent: 0.632812, loss: 1.132466\n",
      "(Epoch): 1800, training data percent: 0.703125, loss: 0.940243\n",
      "(Epoch): 1900, training data percent: 0.664062, loss: 0.947342\n",
      "(Epoch): 2000, training data percent: 0.585938, loss: 1.128682\n",
      "(Epoch): 2100, training data percent: 0.648438, loss: 0.955359\n",
      "(Epoch): 2200, training data percent: 0.578125, loss: 1.200589\n",
      "(Epoch): 2300, training data percent: 0.695312, loss: 0.874352\n",
      "(Epoch): 2400, training data percent: 0.609375, loss: 1.141188\n",
      "(Epoch): 2500, training data percent: 0.671875, loss: 0.940802\n",
      "(Epoch): 2600, training data percent: 0.726562, loss: 0.827346\n",
      "(Epoch): 2700, training data percent: 0.679688, loss: 0.931653\n",
      "(Epoch): 2800, training data percent: 0.664062, loss: 0.934726\n",
      "(Epoch): 2900, training data percent: 0.664062, loss: 0.933113\n",
      "(Epoch): 3000, training data percent: 0.664062, loss: 0.899756\n",
      "(Epoch): 3100, training data percent: 0.718750, loss: 0.849051\n",
      "(Epoch): 3200, training data percent: 0.687500, loss: 0.851007\n",
      "(Epoch): 3300, training data percent: 0.671875, loss: 0.973863\n",
      "(Epoch): 3400, training data percent: 0.718750, loss: 0.713673\n",
      "(Epoch): 3500, training data percent: 0.703125, loss: 0.994726\n",
      "(Epoch): 3600, training data percent: 0.593750, loss: 1.201796\n",
      "(Epoch): 3700, training data percent: 0.539062, loss: 1.371752\n",
      "(Epoch): 3800, training data percent: 0.718750, loss: 0.748356\n",
      "(Epoch): 3900, training data percent: 0.601562, loss: 1.142749\n",
      "(Epoch): 4000, training data percent: 0.734375, loss: 0.838273\n",
      "(Epoch): 4100, training data percent: 0.703125, loss: 0.864954\n",
      "(Epoch): 4200, training data percent: 0.656250, loss: 0.990825\n",
      "(Epoch): 4300, training data percent: 0.640625, loss: 1.062783\n",
      "(Epoch): 4400, training data percent: 0.703125, loss: 0.931920\n",
      "(Epoch): 4500, training data percent: 0.734375, loss: 0.840722\n",
      "(Epoch): 4600, training data percent: 0.734375, loss: 0.781573\n",
      "(Epoch): 4700, training data percent: 0.687500, loss: 0.875613\n",
      "(Epoch): 4800, training data percent: 0.695312, loss: 0.908228\n",
      "(Epoch): 4900, training data percent: 0.648438, loss: 1.072173\n",
      "(Epoch): 5000, training data percent: 0.695312, loss: 0.869522\n",
      "(Epoch): 5100, training data percent: 0.625000, loss: 1.187389\n",
      "(Epoch): 5200, training data percent: 0.679688, loss: 0.984871\n",
      "(Epoch): 5300, training data percent: 0.757812, loss: 0.677846\n",
      "(Epoch): 5400, training data percent: 0.593750, loss: 1.161587\n",
      "(Epoch): 5500, training data percent: 0.664062, loss: 0.907570\n",
      "(Epoch): 5600, training data percent: 0.703125, loss: 1.007320\n",
      "(Epoch): 5700, training data percent: 0.757812, loss: 0.768165\n",
      "(Epoch): 5800, training data percent: 0.640625, loss: 1.072864\n",
      "(Epoch): 5900, training data percent: 0.695312, loss: 0.951094\n",
      "(Epoch): 6000, training data percent: 0.632812, loss: 0.926772\n",
      "(Epoch): 6100, training data percent: 0.617188, loss: 1.060230\n",
      "(Epoch): 6200, training data percent: 0.601562, loss: 1.218215\n",
      "(Epoch): 6300, training data percent: 0.750000, loss: 0.770548\n",
      "(Epoch): 6400, training data percent: 0.671875, loss: 0.967160\n",
      "(Epoch): 6500, training data percent: 0.710938, loss: 1.001310\n",
      "(Epoch): 6600, training data percent: 0.625000, loss: 1.041884\n",
      "(Epoch): 6700, training data percent: 0.687500, loss: 0.880810\n",
      "(Epoch): 6800, training data percent: 0.718750, loss: 0.818110\n",
      "(Epoch): 6900, training data percent: 0.742188, loss: 0.713381\n",
      "(Epoch): 7000, training data percent: 0.718750, loss: 0.771003\n",
      "(Epoch): 7100, training data percent: 0.687500, loss: 0.907156\n",
      "(Epoch): 7200, training data percent: 0.718750, loss: 0.822231\n",
      "(Epoch): 7300, training data percent: 0.734375, loss: 0.661326\n",
      "(Epoch): 7400, training data percent: 0.679688, loss: 1.044966\n",
      "(Epoch): 7500, training data percent: 0.679688, loss: 0.780885\n",
      "(Epoch): 7600, training data percent: 0.656250, loss: 1.251063\n",
      "(Epoch): 7700, training data percent: 0.648438, loss: 0.904149\n",
      "(Epoch): 7800, training data percent: 0.726562, loss: 1.016543\n",
      "(Epoch): 7900, training data percent: 0.648438, loss: 1.005683\n",
      "(Epoch): 8000, training data percent: 0.750000, loss: 0.682084\n",
      "(Epoch): 8100, training data percent: 0.789062, loss: 0.630805\n",
      "(Epoch): 8200, training data percent: 0.718750, loss: 1.037714\n",
      "(Epoch): 8300, training data percent: 0.593750, loss: 1.038308\n",
      "(Epoch): 8400, training data percent: 0.726562, loss: 0.795958\n",
      "(Epoch): 8500, training data percent: 0.718750, loss: 0.929693\n",
      "(Epoch): 8600, training data percent: 0.695312, loss: 0.858119\n",
      "(Epoch): 8700, training data percent: 0.703125, loss: 0.760562\n",
      "(Epoch): 8800, training data percent: 0.742188, loss: 0.848593\n",
      "(Epoch): 8900, training data percent: 0.679688, loss: 0.875585\n",
      "(Epoch): 9000, training data percent: 0.695312, loss: 1.049482\n",
      "(Epoch): 9100, training data percent: 0.687500, loss: 0.897210\n",
      "(Epoch): 9200, training data percent: 0.710938, loss: 0.855084\n",
      "(Epoch): 9300, training data percent: 0.664062, loss: 1.194926\n",
      "(Epoch): 9400, training data percent: 0.671875, loss: 1.000531\n",
      "(Epoch): 9500, training data percent: 0.742188, loss: 0.749380\n",
      "(Epoch): 9600, training data percent: 0.570312, loss: 1.150011\n",
      "(Epoch): 9700, training data percent: 0.750000, loss: 0.886919\n",
      "(Epoch): 9800, training data percent: 0.734375, loss: 0.750587\n",
      "(Epoch): 9900, training data percent: 0.632812, loss: 0.973011\n",
      "test data accuracy: 0.628200\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.datasets.cifar10 import load_data\n",
    "\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "\n",
    "  idx = np.arange(0 , len(data))\n",
    "  np.random.shuffle(idx)\n",
    "  idx = idx[:num]\n",
    "  data_shuffle = [data[ i] for i in idx]\n",
    "  labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "  return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "# Building CNN model. \n",
    "def build_CNN_classifier(x):\n",
    "  # Read images\n",
    "  x_image = x\n",
    "\n",
    "  # First convolutional layer - grayscale RGB image(5,5,3).\n",
    "  W_conv1 = tf.Variable(tf.truncated_normal(shape=[5, 5, 3, 64], stddev=5e-2))\n",
    "  b_conv1 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "  h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "\n",
    "  # first Pooling layer\n",
    "  h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "  # second convolutional layer.\n",
    "  W_conv2 = tf.Variable(tf.truncated_normal(shape=[5, 5, 64, 64], stddev=5e-2))\n",
    "  b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "  h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
    "\n",
    "  # second pooling layer.\n",
    "  h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "  # third convolutional layer\n",
    "  W_conv3 = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], stddev=5e-2))\n",
    "  b_conv3 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "  h_conv3 = tf.nn.relu(tf.nn.conv2d(h_pool2, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3)\n",
    "\n",
    "  # fourth convolutional layer\n",
    "  W_conv4 = tf.Variable(tf.truncated_normal(shape=[3, 3, 128, 128], stddev=5e-2))\n",
    "  b_conv4 = tf.Variable(tf.constant(0.1, shape=[128])) \n",
    "  h_conv4 = tf.nn.relu(tf.nn.conv2d(h_conv3, W_conv4, strides=[1, 1, 1, 1], padding='SAME') + b_conv4)\n",
    "\n",
    "  # fifth convolutional layer\n",
    "  W_conv5 = tf.Variable(tf.truncated_normal(shape=[3, 3, 128, 128], stddev=5e-2))\n",
    "  b_conv5 = tf.Variable(tf.constant(0.1, shape=[128]))\n",
    "  h_conv5 = tf.nn.relu(tf.nn.conv2d(h_conv4, W_conv5, strides=[1, 1, 1, 1], padding='SAME') + b_conv5)\n",
    "\n",
    "  # Fully Connected Layer \n",
    "  W_fc1 = tf.Variable(tf.truncated_normal(shape=[8 * 8 * 128, 384], stddev=5e-2))\n",
    "  b_fc1 = tf.Variable(tf.constant(0.1, shape=[384]))\n",
    "\n",
    "  h_conv5_flat = tf.reshape(h_conv5, [-1, 8*8*128])\n",
    "  h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "\n",
    "  # Dropout  \n",
    "  h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) \n",
    "\n",
    "  # Fully Connected Layer \n",
    "  W_fc2 = tf.Variable(tf.truncated_normal(shape=[384, 10], stddev=5e-2))\n",
    "  b_fc2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "  logits = tf.matmul(h_fc1_drop,W_fc2) + b_fc2\n",
    "  y_pred = tf.nn.softmax(logits)\n",
    "\n",
    "  return y_pred, logits\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Load CIFAR-10 dataset \n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "# scalar  One-hot Encoding \n",
    "y_train_one_hot = tf.squeeze(tf.one_hot(y_train, 10),axis=1)\n",
    "y_test_one_hot = tf.squeeze(tf.one_hot(y_test, 10),axis=1)\n",
    "\n",
    "# Convolutional Neural Networks(CNN) build tensorboard graph.\n",
    "y_pred, logits = build_CNN_classifier(x)\n",
    "\n",
    "# Cross Entropy(loss function), traing by RMSPropOptimizer function.\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "train_step = tf.train.RMSPropOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "# calculation Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# starting training prt.\n",
    "with tf.Session() as sess:\n",
    "   \n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  \n",
    "  # 10000 Step epochs\n",
    "  for i in range(10000):\n",
    "    batch = next_batch(128, x_train, y_train_one_hot.eval())\n",
    "\n",
    "\n",
    "    if i % 100 == 0:\n",
    "      train_accuracy = accuracy.eval(feed_dict={x: batch[0], y: batch[1], keep_prob: 1.0})\n",
    "      loss_print = loss.eval(feed_dict={x: batch[0], y: batch[1], keep_prob: 1.0})\n",
    "\n",
    "      print(\"(Epoch): %d, training data percent: %f, loss: %f\" % (i, train_accuracy, loss_print))\n",
    "    sess.run(train_step, feed_dict={x: batch[0], y: batch[1], keep_prob: 0.8})\n",
    "\n",
    "  \n",
    "  test_accuracy = 0.0  \n",
    "  for i in range(10):\n",
    "    test_batch = next_batch(1000, x_test, y_test_one_hot.eval())\n",
    "    test_accuracy = test_accuracy + accuracy.eval(feed_dict={x: test_batch[0], y: test_batch[1], keep_prob: 1.0})\n",
    "  test_accuracy = test_accuracy / 10;\n",
    "  print(\"test data accuracy: %f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
